1. scaling of mpi with n cores

keep everything constant but the n mpi procs

machine(s)
    dora, zbox

inputfiles
    heavy enough to saturate many procs

plots
    lineplot: runtime vs n_procs with 2 lines: zbox, dora

code settings:
    n_procs = {var}
    #define USE_MPI YES
    #define USE_OPENMP YES
    #define N_OMP_THREADS 4
    #define DEBUG OFF
    #define DO_ASSERTS OFF
    #define GET_LOCAL_ITER_TIME  ON
    #define GET_GLOBAL_ITER_TIME ON
    #define WRITE_TIMING ON
    #define WRITE_INIT_STATE  OFF
    #define WRITE_INTER_STATE ON
    #define WRITE_FINAL_STATE OFF
    #define USE_COLOR NO
    #define COM_METHOD _CM_VEKTOR
    #define LOCATION_PRINT OFF
    #define TRACE_PRINT OFF
    #define DEBUG_PRINT OFF
    #define ERROR_PRINT ON
    #define WARN_PRINT ON
    #define INFO_PRINT ON


2. scaling with memory architecture & cpu frequ

keep everthing const exept the machine it runs on. I think 2 procs with 2 threads each would be great, so we can test 4 core machines
machine(s)

    taurus (Xeon E5-1603 @2.8GHz 4 cores)
    rk_home (Intel Core i5 @ 2.8GHz 4 cores [no HT, 3.3GHz])
    [ rk_laptop1 (Intel Core i5 @ 2.2GHz 2 cores) ]
    rk_laptop2 (Intel Core i7 Q720 @ 1.6GHz 4 cores [HT, 2.8GHz])
    zbox (Xeon E5-2660 8cores @ 2.2 GHz [3GHz])
    dora (Xeon E5-2690 12cores@2.6 GHz [3.5GHz])
    there are many more possible...

inputfiles
    something light, that takes not more than 15min on 4 cores

plots
    scatterplot machine vs runtime

code settings:
    n_procs = 2
    #define USE_MPI YES
    #define USE_OPENMP YES
    #define N_OMP_THREADS 2
    #define DEBUG OFF
    #define DO_ASSERTS OFF
    #define GET_LOCAL_ITER_TIME  ON
    #define GET_GLOBAL_ITER_TIME ON
    #define WRITE_TIMING ON
    #define WRITE_INIT_STATE  OFF
    #define WRITE_INTER_STATE ON
    #define WRITE_FINAL_STATE OFF
    #define USE_COLOR NO
    #define COM_METHOD _CM_VEKTOR
    #define LOCATION_PRINT OFF
    #define TRACE_PRINT OFF
    #define DEBUG_PRINT OFF
    #define ERROR_PRINT ON
    #define WARN_PRINT ON
    #define INFO_PRINT ON


3. Scaling of MPI vs OMP on one node

Test on one single node: compare the performance of increasing n_procs 1...8 vs n_threads 1...8

machine(s)
    zbox

inputfiles
    something in the order of 5-10mins on 8 procs, so like an hour on one core.

plots
    lineplot: n_XXX vs runtime with 2 lines (mpi vs omp)

code settings:
    n_procs = [1...8]
    #define USE_MPI [YES NO]
    #define USE_OPENMP [NO YES]
    #define N_OMP_THREADS [1...8]
    #define DEBUG OFF
    #define DO_ASSERTS OFF
    #define GET_LOCAL_ITER_TIME  ON
    #define GET_GLOBAL_ITER_TIME ON
    #define WRITE_TIMING ON
    #define WRITE_INIT_STATE  OFF
    #define WRITE_INTER_STATE ON
    #define WRITE_FINAL_STATE OFF
    #define USE_COLOR NO
    #define COM_METHOD _CM_VEKTOR
    #define LOCATION_PRINT OFF
    #define TRACE_PRINT OFF
    #define DEBUG_PRINT OFF
    #define ERROR_PRINT ON
    #define WARN_PRINT ON
    #define INFO_PRINT ON

-------------------------------------------------
Other possibilities:

* impact of using MPI Vector and one send vs many sends

